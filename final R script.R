#######
# thesis preperation 
#######

#######
# Week 1 
#######

rm(list = ls()) #clear workspace

setwd("E:/onedrive/Groningen(online)/MADS/thesis/assignment 1") #

Review <- read.csv("yelp_academic_dataset_review_short.csv", header = TRUE)
Business <- read.csv("yelp_academic_dataset_business_short.csv", header = TRUE)
User <- read.csv("yelp_academic_dataset_user.csv", header = TRUE)

head(Review)
head(Business)
head(User)

################
# 0ï¸âƒ£ data cleaning -----
################
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(lubridate)
library(data.table)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(stopwords)
library(wordcloud2)
library(topicmodels)
library(ldatuning)
library(tidytext)
library(hunspell)
library(corrplot)
library(reticulate)
library(lavaan)
library(semPlot)
library(fastDummies)
library(pscl)
library(MASS)
library(car)
library(Metrics)

# Exploration
summary(Business)
colSums(is.na(Business))

# checking time scale
min(Review$date)
max(Review$date)

## 0.1 NAs & empty string removing ----
# It looks like there's missing but there's not, probably because of the "" empty string.
sum(Business == "", na.rm = TRUE)

# Replace empty string with na
Business <- Business %>%
  mutate(across(everything(), ~na_if(trimws(as.character(.)), "")))
summary(Business)
colSums(is.na(Business))	 # businessçš„æŸäº›é¤å…æ²¡æœ‰categoriesè¦å»æ‰,åˆ«çš„naå¯ä»¥ä¸ç®¡(åé¢ç›´æ¥filteré€‰äº†é¤å…)

Review <- Review %>%
  mutate(across(everything(), ~na_if(trimws(as.character(.)), "")))
colSums(is.na(Review))     # reviewå®Œå…¨æ²¡æœ‰ç©ºstringæˆ–è€…NAs

Review <- Review %>% arrange(desc(date))
summary(Review)

## 0.2 Data type transformation ----
Review_clean <- Review %>% mutate(
  across(c(stars,useful,funny,cool),~ as.numeric(.x))) %>% 
  mutate(date = as.Date(date))

Business_clean <- Business %>% mutate(
  across(c(postal_code,latitude,longitude,stars,review_count),~ as.numeric(.x))) %>% 
  rename(business_stars = stars) 

User_clean <- User %>% select(user_id,yelping_since,useful,elite,friends,fans,average_stars) %>% 
  mutate(yelping_since = as.Date(yelping_since),
         elite_years = ifelse(elite == "", 0, str_count(elite, ",") + 1),
         friend_count = ifelse(friends == "", 0, str_count(friends, ",") + 1)) %>% 
  rename(user_avg_useful = useful)

colSums(is.na(Review_clean))
colSums(is.na(Business_clean))
colSums(is.na(User_clean))

## 0.3 create DV (new review per quarter)
Review_clean <- Review_clean %>%
  mutate(
    year = format(as.Date(date), "%Y"),
    month = as.numeric(format(as.Date(date), "%m")),
    quarter = case_when(
      month %in% 1:3 ~ "Q1",
      month %in% 4:6 ~ "Q2",
      month %in% 7:9 ~ "Q3",
      month %in% 10:12 ~ "Q4"
    ),
    year_quarter = paste0(year, "-", quarter)
  ) 


# create new column of the date 3 months after review
Review_clean <- Review_clean %>%
  mutate(date_plus_3m = date %m+% months(3))

# create new df for following calculation
review_growth <- Review_clean %>%
  select(review_id, business_id, date, date_plus_3m) %>% 
  arrange(business_id,date)


# ä½¿ç”¨data.tableè®¡ç®—æ¯æ¡è¯„è®ºåä¸‰ä¸ªæœˆçš„æ–°å¢è¯„è®ºæ•°
setDT(review_growth)
review_growth[, new_reviews_3m := 0]

# å¯¹æ¯ä¸ª business å•ç‹¬å¤„ç†
review_growth[, new_reviews_3m := {
  n_total <- .N
  res <- integer(n_total)
  for (i in 1:n_total) {
    t1 <- date[i]
    t2 <- date_plus_3m[i]
    # æŸ¥æ‰¾è¯¥è¯„è®ºæ—¶é—´åçš„ 3 ä¸ªæœˆå†…çš„è¯„è®ºæ•°ï¼ˆä¸åŒ…å«è‡ªèº«ï¼‰
    res[i] <- sum(date > t1 & date <= t2)
  }
  res
}, by = business_id]

# mergeå›åŸè¡¨æ ¼ä¸­
Review_full <- Review_clean %>%
  left_join(select(review_growth, review_id, new_reviews_3m),
            by = "review_id")

# ç­›é€‰ï¼šä»…ä¿ç•™è¯„è®ºæ—¥æœŸåœ¨ 2021-09-30ï¼ˆå«ï¼‰ä¹‹å‰çš„è®°å½•
Review_full <- Review_full[Review_full$date <= as.Date("2021-09-30"), ]
# ç¡®ä¿ date_plus_3m ä¸è¶…è¿‡æ•°æ®æœ€æ™šæ—¶é—´ï¼Œæ¯”å¦‚ "2021-12-31"
Review_full <- Review_full[Review_full$date_plus_3m <= as.Date("2021-12-31"), ]

# è®¡ç®—ä¼šå‘˜é•¿åº¦(å‘å¸–æ—¥å’Œåˆ›å·æ—¥åªå·®)
Review_full$membership_length <- as.numeric(as.Date(Review_full$date)-as.Date(Review_full$yelping_since))

# è®¡ç®—æ–‡æœ¬çš„å­—æ•°
Review_full$word_count <- sapply(strsplit(Review_full$text, "\\s+"), length)
Review_full$word_count  <-  as.numeric(scale(Review_full$word_count, center = TRUE, scale = TRUE))

write.csv(Review_full,file = "Review_full.csv", row.names = FALSE)  # åˆšç”Ÿæˆå®Œ,åé¢çš„ä»£ç è¦æ¢æˆè¿™ä¸ª
Review_full <- read.csv("Review_full.csv", header = TRUE)


## 0.4 data filtering & merging ----

# check the percentage of restaurant among all industry
Business_clean$contains_restaurant <- grepl("restaurant", Business_clean$categories, ignore.case = TRUE)
sum(Business_clean$contains_restaurant)/nrow(Business_clean)*100

# ç­›é€‰ä¸€ä¸‹,è®©businessé‚£è¾¹åªç•™ä¸‹é¤é¥®,åˆ›ä¸ªæ–°df,(å€¼å¾—æ³¨æ„,ç­›é€‰å®Œé¤å…è¯„è®ºè¿˜æœ‰2m5ä¸ª)
restaurant_business <- Business_clean %>% filter(contains_restaurant)

# åˆå¹¶
restaurant_review <- restaurant_business %>% left_join(Review_full , by = "business_id")
colSums(is.na(restaurant_review))

# æ£€æŸ¥é¤å…çš„æ•°æ®å æ‰€æœ‰æ•°æ®çš„ç™¾åˆ†æ¯”
nrow(restaurant_review)/nrow(Review_clean) # 0.67,ä¹Ÿå°±æ˜¯æ€»è¯„è®ºæ•°çš„67%éƒ½æ˜¯é¤å…çš„è¯„è®º

# ç”±äºå¤§éƒ¨åˆ† user éƒ½æ²¡æœ‰è¯¦ç»†ä¿¡æ¯,åªä¿ç•™æœ‰userä¿¡æ¯çš„æ•°æ®,å› æ­¤ä½¿ç”¨ inner join (ç­›é€‰ååªå‰©ä¸‹äº†17wè¯„è®º)
restaurant_review <- restaurant_review %>% inner_join(User_clean , by = "user_id")
colSums(is.na(restaurant_review))

## 0.5 data exploration ----

# å¯è§†åŒ–æ–°å¢è¯„è®ºçš„åˆ†å¸ƒ
ggplot(restaurant_review, aes(new_reviews_3m))+
  geom_histogram(fill = "#800020", color = "white")+
  geom_text(stat = "bin", aes(label = ..count..), vjust = -0.5, size = 3.2)+
  labs(title = "Distribution of New Reviews Within 3 Months Following Each Review",
       y = "Frequency",
       x = "Number of New Reviews")

colSums(is.na(restaurant_review))

# çœ‹ä¸€ä¸‹å“ªäº›é¤å…æ²¡æœ‰è¯„è®ºå§
sum(is.na(restaurant_review$text))

# çœ‹ä¸€ä¸‹å¤šå°‘ä¸ªç”¨æˆ·ç»™é¤å…ç•™è¨€
sum(n_distinct(restaurant_review$user_id)) 

# çœ‹ä¸€ä¸‹ç”¨æˆ·ç•™è¨€çš„é¢‘ç‡
restaurant_review %>% group_by(user_id) %>% slice(review_count) %>% 
  ggplot(aes(review_count))+
  geom_histogram(fill = "#800020", color = "white")+
  geom_text(stat = "bin", aes(label = ..count..), vjust = -0.5, size = 3)+
  labs(title = "Distribution of Reviews by Each User",
       y = "Frequency",
       x = "Number of Reviews")

# æ£€æŸ¥æ¯ä¸ªå·çš„è¯„è®ºæƒ…å†µå¹¶æ¸…æ´—
restaurant_review %>% group_by(state) %>% 
  summarize(review_count = n())

# ç”±äº state HI/MT/NC å„åªæœ‰ä¸€ä¸ªæ ·æœ¬,å› æ­¤åˆ é™¤è¿™ä¸‰ä¸ªæ ·æœ¬,å¹¶ä¸”åˆ é™¤è¿™ä¸‰åˆ—
# éœ€è¦åˆ é™¤çš„ state åç§°
rare_states <- c("HI", "MT", "NC")

# 1. åˆ é™¤ state ä¸º HI/MT/NC çš„è§‚æµ‹å€¼ï¼ˆå‡è®¾æœ‰åŸå§‹ state åˆ—ï¼‰
restaurant_review <- restaurant_review[!(restaurant_review$state %in% rare_states), ]

write.csv(restaurant_review,file = "restaurant_review.csv", row.names = FALSE)
restaurant_review <- read.csv("restaurant_review.csv",header = TRUE)

#######################################################
# 2ï¸âƒ£ NLP step1 --create dfm--------------------
#######################################################

# create corpus
review_corpus <- corpus(restaurant_review$text)
docnames(review_corpus) <- restaurant_review$review_id  

# tokenization
review_tokens <- tokens(review_corpus,
                        what = "word",
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE,
                        remove_url = TRUE,
                        remove_separators = TRUE)

# remove stop words (ç¡®ä¿ä¸ä¼šæœ‰"of the restaurant"è¿™ç§ngram)
review_tokens <- tokens_remove(review_tokens, stopwords("en"))

# create n-gram
review_tokens <- tokens_ngrams(review_tokens, n = 1:3)

# remove custom stop words
custom_stopwords <- c(
  "just", "one", "us", "also", "got", "came", "can", "well", "even", 
  "always", "first", "come", "wait", "never", "little", "went","definitely")

review_tokens <- tokens_remove(review_tokens, custom_stopwords)

# create another tokens with stemming
review_tokens_stem <- tokens_wordstem(review_tokens)

# create dfm
review_dfm <- dfm(review_tokens)
review_dfm_stem <- dfm(review_tokens_stem)

# åˆ é™¤dfmä¸­ä¸€ä¸ªtokenéƒ½æ²¡æœ‰çš„è§‚æµ‹å€¼
kept_review_ids <- docnames(review_dfm)[rowSums(review_dfm) > 0]
kept_stem_review_ids <- docnames(review_dfm)[rowSums(review_dfm) > 0]
setequal(kept_review_ids, kept_stem_review_ids) # æ£€æŸ¥åˆ°ä¸¤ä¸ªæ‰€ç­›é€‰æ‰çš„æ•°æ®éƒ½æ˜¯ä¸€æ ·çš„,åé¢åªç”¨ä¸€ä¸ªå°±å¥½äº†

write.csv(kept_review_ids,file = "kept_review_ids.csv", row.names = FALSE)
kept_review_ids <- read.csv("kept_review_ids.csv",header = TRUE)

review_dfm <- review_dfm[kept_review_ids, ]
review_dfm_stem <- review_dfm_stem[kept_review_ids, ]

# ä¿å­˜dfm,æ–¹ä¾¿åç»­ç›´æ¥ä½¿ç”¨
saveRDS(review_dfm, file = "review_dfm.rds")
saveRDS(review_dfm_stem, file = "review_dfm_stem.rds")
review_dfm <- readRDS("review_dfm.rds")
review_dfm_stem <- readRDS("review_dfm_stem.rds")

# åŒæ­¥åˆ é™¤åŸå§‹æ•°æ®ä¸­çš„åºŸè§‚æµ‹å€¼
restaurant_review_clean <- restaurant_review %>% 
  filter(review_id %in% kept_review_ids)

# å¯¹æ¸…ç†åçš„åŸå§‹æ•°æ®è¿›è¡Œä¿å­˜
write.csv(restaurant_review_clean,file = "restaurant_review_clean.csv", row.names = FALSE)
restaurant_review_clean <- read.csv("restaurant_review_clean.csv",header = TRUE)

# çœ‹å¸¸è§è¯
top_words <- textstat_frequency(review_dfm, n = 100)
top_words_stem <- textstat_frequency(review_dfm_stem, n = 100)

# visualization of high frequency words
ggplot(head(top_words,20), aes(x = reorder(feature, frequency), y = frequency)) +
  geom_col(fill = "#800020") +
  coord_flip() +  
  labs(title = "Top 20 Most Frequent Features",
       x = "Feature",
       y = "Frequency") +
  theme_minimal()

ggplot(head(top_words_stem,20), aes(x = reorder(feature, frequency), y = frequency)) +
  geom_col(fill = "#800020") +
  coord_flip() +  
  labs(title = "Top 20 Most Frequent Features (stem)",
       x = "Feature",
       y = "Frequency") +
  theme_minimal()

#######################################################
# 3ï¸âƒ£ NLP step 2-- LDA ä¸»é¢˜åˆ†æ ä½¿ç”¨stemè¿‡çš„ --------------
#######################################################

# å…ˆå–æ ·æœ¬
set.seed(1234)
sampled_ids <- sample(kept_review_ids, size = 5000)
review_sample <- restaurant_review_clean %>% filter(review_id %in% sampled_ids)
review_dfm_stem_sample <- review_dfm_stem[sampled_ids, ]

# trim dfm
review_dfm_stem_sample_trim <- dfm_trim(review_dfm_stem_sample, min_termfreq = 5)

# convert dfm to dtm
dtm_stem_sample <- convert(review_dfm_stem_sample_trim, to = "topicmodels")

# ä¿å­˜dtm,æ–¹ä¾¿åç»­ç›´æ¥ä½¿ç”¨
saveRDS(dtm_stem_sample, file = "dtm_stem_sample.rds")
dtm_stem_sample <- readRDS("dtm_stem_sample.rds")

# å†³å®šä¸»é¢˜æ•°é‡ (è¿™é‡Œæœ‰ä¸ªé—®é¢˜,ldatuningè¿™ä¸ªåŒ…éœ€è¦R4.3,å‡çº§å®Œä¹‹åç”¨ä¸äº†ldatuning)
result <- FindTopicsNumber(
  dtm_stem_sample,
  topics = seq(2, 30, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 1,
  verbose = TRUE
)

FindTopicsNumber_plot(result)

# perform topic modeling 
lda_model <- LDA(dtm_stem_sample, k = 17, method = "Gibbs",
                 control = list(seed = 1234))


saveRDS(lda_model, file = "lda_model.rds")
lda_model <- readRDS("lda_model.rds")

# çœ‹ä¸»é¢˜çš„å‡ºç°æ¬¡æ•°
# Extract the gamma matrix 
gamma_values <- tidy(lda_model, matrix = "gamma")

# Create grouped gamma tibble
grouped_gammas <- gamma_values %>%
  group_by(document) %>%
  arrange(desc(gamma)) %>%
  slice(1) %>%
  group_by(topic)
# Count (tally) by topic
grouped_gammas %>% 
  tally(topic, sort=TRUE)

# çœ‹æ¯ä¸ªtopicçš„é«˜é¢‘è¯
# Extract top words per topic
top_terms <- tidy(lda_model, matrix = "beta") %>%
  group_by(topic) %>%
  slice_max(order_by = beta, n = 20) %>%  # æ¯ä¸ªä¸»é¢˜å–å‰20ä¸ªè¯
  ungroup()

# Print the top words per topic
print(top_terms)

#  Visualize the Results
# Barplot of top words per topic
top_terms %>%
  ggplot(aes(x = reorder_within(term, beta, topic),
             y = beta,
             fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +  
  labs(title = "Top Words in Each Topic",
       x = "Terms", y = "Beta Score") +
  theme_minimal()


# ç»™topicå‘½å
topic_labels <- c(
  "Tp1_Recommendation_and_Service",          # make, friend, recommend, always
  "Tp2_Dinner_and_Steak",                    # restaur, dinner, meat, steak
  "Tp3_Return_and_Impression",              # tri, place, back, come
  "Tp4_Price_and_Quality",                   # price, fresh, portion, quality
  "Tp5_Menu_and_Restaurant",                 # menu, restaur, option, item
  "Tp6_Breakfast_and_Bacon",                 # breakfast, bacon, egg, sausage
  "Tp7_Timing_and_Arrival",                  # time, now, early, day
  "Tp8_Order_and_Waffle",                    # order, get, waffle, wait
  "Tp9_Positive_Emotion",                    # good, love, amazing
  "Tp10_Burgers_and_Fries",                  # chicken, fri, burger, sandwich
  "Tp11_Soups_and_Asian_Food",              # dish, soup, noodle, asian
  "Tp12_Great_Atmosphere_and_Staff",         # great, friend, recommend, atmosphere
  "Tp13_Pizza_and_Cheesesteak",              # pizza, salad, cheesesteak
  "Tp14_Mexican_Flavors",                    # flavor, taco, mexican
  "Tp15_Bar_and_Drinks",                     # bar, dinner, drink
  "Tp16_Local_and_place",                    # local, food, clear
  "Tp17_Subjective_Feelings"                # like, feel, though
)

# ç›¸å¯¹æ¥è¯´,ä¸€ä¸‹è¯é¢˜ä¸é¤å…æœ¬èº«æ¯”è¾ƒæ— å…³
# Tp8 å†…å®¹åæ³›åŒ–è¤’å¥–ï¼Œå¦‚ love, best, amazingï¼›ç¼ºä¹ç»†èŠ‚
# Tp13 like, want, feel ç­‰è¯ä¸»è§‚æƒ…æ„Ÿå¼ºã€ä¸ç‰¹æŒ‡é¤å…æœåŠ¡/èœå“
# Tp15 like, go, eat, want, feel ç­‰æ³›æ³›è€Œè°ˆï¼Œä¸å…·ä½“æŒ‡æ¶‰èœå•/æœåŠ¡ç­‰
# ä»¥ä¸Štpè”åˆå…¶ä»–è¯é¢˜éƒ½èƒ½æœ‰ç”¨,ä½†æ˜¯å¦‚æœåªæœ‰ä»¥ä¸Šæ˜¾å½¢æ˜¯å°†è®¤ä¸ºæ˜¯irrelevant

# æå–ä¸»é¢˜åˆ†å¸ƒ
theta_all <- as.data.frame(posterior(lda_model)$topics)
colnames(theta_all) <- topic_labels

# æ·»åŠ  review_id å¹¶åˆå¹¶
sample_topic <- cbind(review_sample,theta_all)

write.csv(sample_topic,file = "sample_topic.csv", row.names = FALSE)
sample_topic <- read.csv("sample_topic.csv",header = TRUE)

# é€šè¿‡è¯é¢˜ç™¾åˆ†æ¯”å’ŒåŠ æƒæ¥è®¡ç®—ç›¸å…³æ€§ ----
topic_weights <- c(
  1.0,  # Tp1ï¼šæ¨è + æœåŠ¡
  1.0,  # Tp2ï¼šæ™šé¤ä¸»èœ
  0.8,  # Tp3ï¼šé¡¾å®¢å›è®¿
  1.0,  # Tp4ï¼šé£Ÿç‰©ä»·æ ¼å’Œè´¨é‡
  0.8,  # Tp5ï¼šèœå•å†…å®¹
  1.0,  # Tp6ï¼šæ—©é¤ä¸»é£Ÿ
  0.6,  # Tp7ï¼šæ—¶é—´/åˆ°è¾¾æ—¶é—´
  0.8,  # Tp8ï¼šç‚¹å•æµç¨‹
  0.6,  # Tp9ï¼šç§¯ææƒ…æ„Ÿï¼ˆä¸å…·ä½“ï¼‰
  1.0,  # Tp10ï¼šæ±‰å ¡è–¯æ¡
  1.0,  # Tp11ï¼šäºšæ´²èœ/æ±¤ç±»
  1.0,  # Tp12ï¼šæ°›å›´ + æœåŠ¡
  1.0,  # Tp13ï¼šæŠ«è¨/å¥¶é…ªä¸‰æ˜æ²»
  1.0,  # Tp14ï¼šå¢¨è¥¿å“¥å£å‘³
  0.8,  # Tp15ï¼šé…’å§é¥®å“
  0.6,  # Tp16ï¼šæœ¬åœ°/åœ°ç‚¹
  0.6   # Tp17ï¼šä¸»è§‚æ„Ÿå—åæ³›
)

## æ‰¾å‡º LDA åˆ†å¸ƒåˆ—å
topic_cols <- grep("^Tp", names(sample_topic), value = TRUE)

# è®¡ç®— relevance score
sample_topic$relevant <- as.numeric(as.matrix(sample_topic[, topic_cols]) %*% topic_weights)

ggplot(sample_topic,aes(relevant))+
  geom_histogram(fill= "#800020",color="white")+
  labs(title = "Distribution of Relevance of Each Review",
       y = "Frequency",
       x = "relevant")

#ä»¥topic 1/2ä¸ºä¾‹å­è§‚æµ‹åˆ†å¸ƒé¢‘ç‡
ggplot(sample_topic,aes(Tp1_Recommendation_and_Service))+
  geom_histogram(fill= "#800020",color="white")
ggplot(sample_topic,aes(Tp2_Dinner_and_Steak))+
  geom_histogram(fill= "#800020",color="white")

# é€šè¿‡é˜ˆå€¼è®¾ç½®å°†TPå˜æˆboolean 
summary(sample_topic)

# å°†æ‰€æœ‰ä¸»é¢˜äºŒå€¼åŒ–ï¼ˆ>= 0.08ï¼‰
for (tp in topic_labels) {
  sample_topic[[tp]] <- if_else(sample_topic[[tp]] >= 0.08, 1, 0)
}

# comprehensive = ä¸»é¢˜ç™¾åˆ†æ¯”è¶…è¿‡é˜ˆå€¼çš„ä¸»é¢˜æ•°é‡ä¹‹å’Œ
sample_topic <- sample_topic %>%
  mutate(comprehensive = rowSums(across(all_of(topic_labels))))

ggplot(sample_topic,aes(comprehensive))+
  geom_bar(fill= "#800020",color="white")+
  geom_text(stat = "count", aes(label = after_stat(count)), 
            vjust = -0.5, size = 3.2)+
  labs(title = "Distribution of comprehensive of Each Review",
       y = "Frequency",
       x = "Number of topics")

sample_topic <- sample_topic %>% select(-Unnamed..0)

sample_step1_done <- sample_topic 
colSums(is.na(sample_step1_done))

# reviewçš„clarity
spelling_errors <- hunspell(sample_step1_done$text)
error_count <- sapply(spelling_errors, length)
word_count <- sapply(strsplit(sample_step1_done$text, "\\s+"), length)
spelling_error_rate <- error_count / word_count
sample_step1_done$clarity <- 1-spelling_error_rate

write.csv(sample_step1_done,file = "sample_step1_done.csv", row.names = FALSE)
sample_step1_done <- read.csv("sample_step1_done.csv",header = TRUE)

#######################################################
# 4ï¸âƒ£ NLP step 3-- sentiment åˆ†æ
#######################################################

## å·²ç»è®­ç»ƒå¥½çš„æ·±åº¦å­¦ä¹ (BERT)çš„NLPæ¨¡å‹ ------------

# å®‰è£… reticulate åŒ…
#install.packages("reticulate")
library(reticulate)

#reticulate::install_miniconda()
py_install(c("transformers", "torch"))

# 2. åŠ è½½ Python æ¨¡å‹
transformers <- import("transformers")
torch <- import("torch")

tokenizer <- transformers$AutoTokenizer$from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment")
model <- transformers$AutoModelForSequenceClassification$from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment")

reticulate::py_run_string("
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F

tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')

def predict_single_sentiment(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = F.softmax(outputs.logits, dim=1)
        probs = probs.numpy().flatten()
        label = probs.argmax() + 1  # labels are 1-indexed
    return label, probs.tolist()
")

# è®¾ç½®è¿›è¡ŒBertæƒ…æ„Ÿåˆ†æçš„å‡½æ•°
predict_sentiment <- function(texts) {
  results <- lapply(texts, function(txt) {
    out <- reticulate::py$predict_single_sentiment(txt)
    list(label = out[[1]], probs = unlist(out[[2]]))
  })
  return(results)
}

# ä½¿ç”¨bertè¿›è¡Œæƒ…æ„Ÿåˆ†æ
texts <- sample_step1_done$text
sentiment_results <- predict_sentiment(texts)

saveRDS(sentiment_results, file = "sentiment_results.rds")
sentiment_results <- readRDS("sentiment_results.rds")

sample_step1_done <- sample_step1_done %>%
  mutate(
    bert_score = sapply(sentiment_results, function(x) x$label),
    prob_1star = sapply(sentiment_results, function(x) x$probs[1]),
    prob_2star = sapply(sentiment_results, function(x) x$probs[2]),
    prob_3star = sapply(sentiment_results, function(x) x$probs[3]),
    prob_4star = sapply(sentiment_results, function(x) x$probs[4]),
    prob_5star = sapply(sentiment_results, function(x) x$probs[5])
  )
# ä½¿ç”¨bert scoreæ¥æobjective
sample_step1_done$bert_objective <- 1 - (abs(sample_step1_done$bert_score - 3) / 2)
# å› ä¸ºåœ¨CFAåˆ†æçš„åœ°æ–¹éœ€è¦linearä¸€ç‚¹çš„å˜é‡,å› æ­¤é€šè¿‡jitterä½¿å˜é‡çš„å€¼å˜å¤š
set.seed(1234) 
sample_step1_done$bert_objective <- jitter(sample_step1_done$bert_objective, amount = 0.05)

sample_step1_done <- sample_step1_done %>%
  select(-prob_1star, -prob_2star, -prob_3star, -prob_4star, -prob_5star)

sample_step2_done <- sample_step1_done

ggplot(sample_step2_done,aes(bert_score))+
  geom_bar(fill= "#800020",color="white")+
  geom_text(stat = "count", aes(label = after_stat(count)), 
            vjust = -0.5, size = 3.2)+
  labs(title = "Distribution of bert_score",
       y = "Frequency",
       x = "bert_score")

# ä½¿ç”¨bert|afinnæ¥åŒºåˆ†valence positive(1)/neutral(0)/negative(-1)
sample_step2_done <- sample_step2_done %>% 
  mutate(bert_valence = case_when(bert_score > 3 ~  1,
                                  bert_score < 3 ~ -1,
                                  bert_score ==3 ~  0,
                                  TRUE ~ NA)) 

summary(sample_step2_done[, c("comprehensive", "clarity", "bert_score", "bert_objective", "bert_valence")])

write.csv(sample_step2_done,file = "sample_step2_done.csv", row.names = FALSE)
sample_step2_done <- read.csv("sample_step2_done.csv",header = TRUE)

#######################################################
# 5ï¸âƒ£ Descriptive Analysis --------------
#######################################################
sample_step2_done <- sample_step2_done %>% 
  select(-address,-postal_code,-latitude,-longitude,-is_open,
         -attributes,-hours)

colSums(is.na(sample_step2_done))
summary(sample_step2_done[, c(
  "useful", "new_reviews_3m", 
  "relevant", "comprehensive", "clarity", "bert_score", 
  "bert_objective", "bert_valence", "membership_length", "user_avg_useful"
)])

summary(sample_step2_done %>% select(user_id,user_avg_useful,fans,average_stars,elite_years,friend_count) %>%
          distinct(user_id, .keep_all = TRUE) )

# visualization
df_long <- sample_step2_done %>%
  select(all_of(c("useful", "new_reviews_3m", 
                  "relevant", "comprehensive", "clarity", "bert_score", 
                  "bert_objective", "bert_valence", "membership_length",
                  "user_avg_useful","average_stars","elite_years"))) %>%
  pivot_longer(cols = everything(), 
               names_to = "variable", 
               values_to = "value")

summary_counts <- sample_step2_done %>%
  summarise(
    total_reviews = n(),
    unique_businesses = n_distinct(business_id),
    unique_users = n_distinct(user_id),
    unique_states = n_distinct(state)
  )
summary_counts

# ç»˜åˆ¶å¤šå˜é‡çš„ç‹¬ç«‹ boxplotï¼ˆæ¯ä¸ªå˜é‡ä¸€ä¸ªå°å›¾ï¼‰
ggplot(df_long, aes(x = "", y = value)) +
  geom_boxplot() +
  facet_wrap(~ variable, scales = "free_y") +  # æ¯ä¸ªå°å›¾ç‹¬ç«‹åæ ‡è½´
  theme_minimal() +
  labs(title = "Boxplots of Each Variable (with Independent Y Scales)", x = "", y = "") +
  theme(strip.text = element_text(size = 10))

# correlation plot
# é€‰æ‹©æŒ‡å®šåˆ—
selected_cols <- sample_step2_done %>%
  select(as.numeric(all_of(c("useful", "new_reviews_3m", 
                             "relevant", "comprehensive", "clarity", "bert_objective", "word_count", 
                             "bert_valence", "membership_length","elite_years", "user_avg_useful")))) 

# è®¡ç®—æ¯ä¸ªå˜é‡çš„å‡å€¼å’Œæ ‡å‡†å·®
descriptive_stats <- sample_step2_done %>%
  select(all_of(c("useful", "new_reviews_3m", 
                  "relevant", "comprehensive", "clarity", "bert_score", 
                  "bert_objective", "bert_valence", "membership_length",
                  "user_avg_useful", "average_stars", "elite_years"))) %>%
  summarise(across(everything(), list(mean = ~mean(., na.rm = TRUE),
                                      sd = ~sd(., na.rm = TRUE))))
descriptive_stats

# å°†é€»è¾‘å‹å˜é‡è½¬æ¢ä¸ºæ•°å€¼å‹
selected_cols <- selected_cols %>%
  mutate(across(where(is.logical), as.numeric))

# è®¡ç®—ç›¸å…³çŸ©é˜µï¼ˆè‡ªåŠ¨æ’é™¤ NAï¼‰
cor_matrix <- cor(selected_cols, use = "pairwise.complete.obs")

# ç»˜åˆ¶ç›¸å…³å›¾
corrplot(cor_matrix, method = "color", type = "upper",
         tl.col = "black", tl.srt = 45, addCoef.col = "black",
         number.cex = 0.6, col = colorRampPalette(c("steelblue", "white", "tomato"))(200))

sample_step3_done <- sample_step2_done

write.csv(sample_step3_done, "sample_step3_done.csv", row.names = FALSE)
sample_step3_done <- read.csv("sample_step3_done.csv",header = TRUE)

#######################################################
# 6ï¸âƒ£ SEM Modeling --------------
#######################################################
#â€œInstead of using automated cross-validation, this study applied five-fold random subsetting to 
#manually assess model stability across samples. The approach allowed direct inspection of CFA loadings 
#and SEM paths, offering transparent robustness checks.â€
# =============================================
# âœ… 1. æ•°æ®å‡†å¤‡ä¸ä¸­å¿ƒåŒ–å˜é‡ + äº¤äº’é¡¹æ„å»º
# =============================================

# åˆ›å»ºuseful_dummy (é¿å…ç”±äºusefulçš„é›¶è†¨èƒ€å’Œskewç‰¹å¾å½±å“semçš„ä¼°è®¡)
sample_step3_done <- sample_step3_done %>% mutate(
  useful_dummy = if_else(useful==0,0,1))

# äº¤äº’é¡¹åˆ›å»ºä¸­å¿ƒåŒ–å˜é‡
sample_step3_done <- sample_step3_done %>%
  mutate(
    valence_c = scale(bert_valence, center = TRUE, scale = FALSE),
    int_use_val = useful_dummy * valence_c
  )

# è§‚æµ‹å˜é‡è¿›è¡Œæ ‡å‡†åŒ–
sample_step3_done <- sample_step3_done %>%
  mutate(
    clarity = as.numeric(scale(clarity, center = TRUE, scale = TRUE)),
    comprehensive = as.numeric(scale(comprehensive, center = TRUE, scale = TRUE)),
    user_avg_useful = as.numeric(scale(user_avg_useful, center = TRUE, scale = TRUE)),
    elite_years = as.numeric(scale(elite_years, center = TRUE, scale = TRUE)),
    membership_length = as.numeric(scale(membership_length, center = TRUE, scale = TRUE)),
    bert_objective = as.numeric(scale(bert_objective, center = TRUE, scale = TRUE)),
    stars = as.numeric(scale(stars, center = TRUE, scale = TRUE)),
    review_count = as.numeric(scale(review_count, center = TRUE, scale = TRUE)),
    relevant = as.numeric(scale(relevant, center = TRUE, scale = TRUE))
  )

# å¯¹å› å˜é‡è¿›è¡Œlogå¤„ç†
sample_step3_done$new_reviews_3m_ln <- log1p(sample_step3_done$new_reviews_3m)

# å¯¹æ–‡å­—å˜é‡è¿›è¡Œfactorå¤„ç†
sample_step3_done$name <- as.factor(sample_step3_done$name)
sample_step3_done$state <- as.factor(sample_step3_done$state)
sample_step3_done$year_quarter <- as.factor(sample_step3_done$year_quarter)
summary(sample_step3_done)

# ===============================
# Step 1: æ‹†åˆ†æ•°æ®
# ===============================
set.seed(123)
n <- nrow(sample_step3_done)
split_indices <- split(1:n, cut(1:n, breaks = 5, labels = FALSE))
data_list <- lapply(split_indices, function(idx) sample_step3_done[idx, ])

# ===============================
# Step 2: å¾ªç¯CFAåˆ†æ
# ===============================
# åˆ›å»ºç©ºæ•°æ®åº“
cfa_results <- list()
cfa_fitmeasures <- data.frame()
cfa_results_plot <- list()

for (i in 1:5) {
  data_i <- data_list[[i]]
  
  cfa_model <- '
    Argument_Quality =~ clarity  +  word_count + relevant + bert_objective + comprehensive
    Source_Credibility =~ user_avg_useful + elite_years + membership_length
  '
  
  fit_cfa <- cfa(cfa_model, data = data_i, std.lv = TRUE)
  cfa_results[[i]] <- parameterEstimates(fit_cfa, standardized = TRUE)
  
  cfa_results_plot[[i]] <- fit_cfa
  
  fitm <- fitMeasures(fit_cfa, c("cfi", "tli", "rmsea", "srmr"))
  cfa_fitmeasures <- rbind(cfa_fitmeasures, 
                           data.frame(set = i,
                                      cfi = fitm["cfi"],
                                      tli = fitm["tli"],
                                      rmsea = fitm["rmsea"],
                                      srmr = fitm["srmr"]))
}

# å‚æ•°ç»“æœ
cfa_results[[1]]
cfa_results[[2]]
cfa_results[[3]]
cfa_results[[4]]
cfa_results[[5]]

# æ‹ŸåˆæŒ‡æ ‡
cfa_fitmeasures 

# å¯è§†åŒ–
for (i in seq_along(cfa_results_plot)) {
  cat("\nğŸ“Š æ­£åœ¨æ˜¾ç¤ºç¬¬", i, "ä¸ªæ•°æ®å­é›†çš„ SEM è·¯å¾„å›¾...\n")
  
  semPaths(
    cfa_results_plot[[i]],
    what = "std",              # ä½¿ç”¨æ ‡å‡†åŒ–è·¯å¾„ç³»æ•°
    layout = "tree",           # æ ‘çŠ¶ç»“æ„
    style = "lisrel",          # LISREL é£æ ¼
    nCharNodes = 0,            # ä¸æˆªæ–­å˜é‡å
    residuals = FALSE,         # ä¸æ˜¾ç¤ºæ®‹å·®ç®­å¤´
    title = FALSE,             # ä¸æ˜¾ç¤ºæ ‡é¢˜
    #    fade = FALSE
    #    ,              # å–æ¶ˆé€æ˜åº¦æ·¡åŒ–
    sig = 0.05,                  # æ˜¾è‘—æ€§æ ‡æ˜Ÿå·é˜ˆå€¼
    edge.label.sig = TRUE        # æ·»åŠ æ˜¾è‘—æ€§
  )
  
  # å¯é€‰ï¼šæš‚åœæŸ¥çœ‹ï¼ˆæŒ‰å›è½¦ç»§ç»­ï¼‰
  readline(prompt = "æŒ‰å›è½¦æŸ¥çœ‹ä¸‹ä¸€ä¸ªå›¾...")
}

# ===============================
# Step 4: å¾ªç¯SEM_1åˆ†æ
# ===============================

# åˆ›å»ºç©ºæ•°æ®æ¡†
sem_results <- list()
sem_results_plot <- list()
sem_fitmeasures <- data.frame()

for (i in 1:5) {
  data_i <- data_list[[i]]
  
  sem_model <- '
    Argument_Quality =~ clarity  +  word_count + relevant + bert_objective + comprehensive
    Source_Credibility =~ user_avg_useful + elite_years + membership_length
    useful_dummy ~ Argument_Quality + Source_Credibility
  '
  #due to the non-normality of useful, I choosed MLR a Robust Maximum Likelihood method
  fit_sem <- sem(sem_model, data = data_i, std.lv = TRUE, estimator = "MLR") 
  
  sem_results[[i]] <- parameterEstimates(fit_sem, standardized = TRUE)
  # âœ” ä¿å­˜å®Œæ•´æ¨¡å‹å¯¹è±¡ï¼ˆä¸æ˜¯summaryæˆ–å‚æ•°ï¼‰
  sem_results_plot[[i]] <- fit_sem
  
  # âœ” åŒæ—¶å­˜fitæŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
  fitm <- fitMeasures(fit_sem, c("cfi", "tli", "rmsea", "srmr"))
  sem_fitmeasures <- rbind(sem_fitmeasures,
                           data.frame(set = i,
                                      cfi = fitm["cfi"],
                                      tli = fitm["tli"],
                                      rmsea = fitm["rmsea"],
                                      srmr = fitm["srmr"]))
}

# æ‹ŸåˆæŒ‡æ ‡
sem_fitmeasures 

# å‚æ•°ç»“æœ
sem_results[[1]]
sem_results[[2]]
sem_results[[3]]
sem_results[[4]]
sem_results[[5]]

# å¯è§†åŒ–
for (i in seq_along(sem_results_plot)) {
  cat("\nğŸ“Š æ­£åœ¨æ˜¾ç¤ºç¬¬", i, "ä¸ªæ•°æ®å­é›†çš„ SEM è·¯å¾„å›¾...\n")
  
  semPaths(
    sem_results_plot[[i]],
    what = "std",              # ä½¿ç”¨æ ‡å‡†åŒ–è·¯å¾„ç³»æ•°
    layout = "tree",           # æ ‘çŠ¶ç»“æ„
    style = "lisrel",          # LISREL é£æ ¼
    nCharNodes = 0,            # ä¸æˆªæ–­å˜é‡å
    residuals = FALSE,         # ä¸æ˜¾ç¤ºæ®‹å·®ç®­å¤´
    title = FALSE,             # ä¸æ˜¾ç¤ºæ ‡é¢˜
    #    fade = FALSE
    #    ,              # å–æ¶ˆé€æ˜åº¦æ·¡åŒ–
    sig = 0.05,                  # æ˜¾è‘—æ€§æ ‡æ˜Ÿå·é˜ˆå€¼
    edge.label.sig = TRUE        # æ·»åŠ æ˜¾è‘—æ€§
  )
  
  # å¯é€‰ï¼šæš‚åœæŸ¥çœ‹ï¼ˆæŒ‰å›è½¦ç»§ç»­ï¼‰
  readline(prompt = "æŒ‰å›è½¦æŸ¥çœ‹ä¸‹ä¸€ä¸ªå›¾...")
}

#################################
# Step 5: å°†å®Œæ•´æ¨¡å‹æ”¾å…¥ SEM_2
#################################
# åˆå§‹åŒ–åˆ—è¡¨
sem_full_results <- list()
sem_full_results_plot <- list()
sem_full_fitmeasures <- data.frame()


for (i in 1:5) {
  data_i <- data_list[[i]]
  
  
  # SEMæ¨¡å‹ï¼šåŒ…å« useful â†’ new_reviews_3m çš„ç»“æ„è·¯å¾„
  sem_model_full <- '
    # Measurement model
    Argument_Quality =~ clarity + word_count + relevant + bert_objective + comprehensive
    Source_Credibility =~ user_avg_useful + elite_years + membership_length

    # Structural model
    useful_dummy ~ Argument_Quality + Source_Credibility
    new_reviews_3m_ln ~ useful_dummy + valence_c + int_use_val + review_count
  '
  
  # âš  ä½¿ç”¨ MLR åšé²æ£’ä¼°è®¡ï¼Œå› åŒ…å«è®¡æ•°å‹æ•°æ®
  fit_sem_full <- sem(sem_model_full, data = data_i, std.lv = TRUE, estimator = "MLR")
  
  # ä¿å­˜ç»“æœ
  sem_full_results[[i]] <- parameterEstimates(fit_sem_full, standardized = TRUE)
  sem_full_results_plot[[i]] <- fit_sem_full
  
  # æå–æ‹ŸåˆæŒ‡æ ‡
  fitm <- fitMeasures(fit_sem_full, c("cfi", "tli", "rmsea", "srmr"))
  sem_full_fitmeasures <- rbind(sem_full_fitmeasures,
                                data.frame(set = i,
                                           cfi = fitm["cfi"],
                                           tli = fitm["tli"],
                                           rmsea = fitm["rmsea"],
                                           srmr = fitm["srmr"]))
}

# æ‹ŸåˆæŒ‡æ ‡
sem_full_fitmeasures

# å‚æ•°ç»“æœ
sem_full_results[[1]]
sem_full_results[[2]]
sem_full_results[[3]]
sem_full_results[[4]]
sem_full_results[[5]]


for (i in seq_along(sem_full_results_plot)) {
  cat("\nğŸ“Š æ­£åœ¨æ˜¾ç¤ºç¬¬", i, "ä¸ªå­é›†çš„ SEM å…¨è·¯å¾„å›¾...\n")
  
  semPaths(
    sem_full_results_plot[[i]],
    what = "std",              # ä½¿ç”¨æ ‡å‡†åŒ–è·¯å¾„ç³»æ•°
    layout = "tree",           # æ ‘çŠ¶ç»“æ„
    style = "lisrel",          # LISREL é£æ ¼
    nCharNodes = 0,            # ä¸æˆªæ–­å˜é‡å
    residuals = FALSE,         # ä¸æ˜¾ç¤ºæ®‹å·®ç®­å¤´
    title = FALSE,             # ä¸æ˜¾ç¤ºæ ‡é¢˜
    #    fade = FALSE
    #    ,              # å–æ¶ˆé€æ˜åº¦æ·¡åŒ–
    sig = 0.05,                  # æ˜¾è‘—æ€§æ ‡æ˜Ÿå·é˜ˆå€¼
    edge.label.sig = TRUE        # æ·»åŠ æ˜¾è‘—æ€§
  )
  
  readline(prompt = "æŒ‰å›è½¦æŸ¥çœ‹ä¸‹ä¸€ä¸ªå›¾...")
}


summary(sample_step3_done)
sd(sample_step3_done$word_count)
mean(sample_step3_done$word_count)

#######################################################
# 7ï¸âƒ£ è´ŸäºŒé¡¹å›å½’(ä½¿ç”¨å®Œæ•´æ•°æ®é›†) --------------
#######################################################


## å·²ç»è®­ç»ƒå¥½çš„æ·±åº¦å­¦ä¹ (BERT)çš„NLPæ¨¡å‹ ------------
# å®‰è£… reticulate åŒ…
#install.packages("reticulate")
library(reticulate)

#reticulate::install_miniconda()

py_install(c("transformers", "torch"))

# 2. åŠ è½½ Python æ¨¡å‹
transformers <- import("transformers")
torch <- import("torch")

tokenizer <- transformers$AutoTokenizer$from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment")
model <- transformers$AutoModelForSequenceClassification$from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment")

reticulate::py_run_string("
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F

tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')

def predict_single_sentiment(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = F.softmax(outputs.logits, dim=1)
        probs = probs.numpy().flatten()
        label = probs.argmax() + 1  # labels are 1-indexed
    return label, probs.tolist()
")


predict_sentiment <- function(texts) {
  results <- lapply(texts, function(txt) {
    out <- reticulate::py$predict_single_sentiment(txt)
    list(label = out[[1]], probs = unlist(out[[2]]))
  })
  return(results)
}

texts <- restaurant_review_clean$text

# ä½¿ç”¨bertæ¥åˆ†æsentiment
sentiment_results_full <- predict_sentiment(texts)
restaurant_review_full_with_bert <- restaurant_review %>%
  mutate(
    bert_score = sapply(sentiment_results_full, function(x) x$label),
    prob_1star = sapply(sentiment_results_full, function(x) x$probs[1]),
    prob_2star = sapply(sentiment_results_full, function(x) x$probs[2]),
    prob_3star = sapply(sentiment_results_full, function(x) x$probs[3]),
    prob_4star = sapply(sentiment_results_full, function(x) x$probs[4]),
    prob_5star = sapply(sentiment_results_full, function(x) x$probs[5])
  ) 

# ä½¿ç”¨bert scoreæ¥æ„å»ºobjectiveness
restaurant_review_full_with_bert$bert_objective <- 1 - (abs(restaurant_review_full_with_bert$bert_score - 3) / 2)
set.seed(1234)
restaurant_review_full_with_bert$bert_objective <- jitter(restaurant_review_full_with_bert$bert_objective, amount = 0.05)

restaurant_review_full_with_bert <- restaurant_review_full_with_bert %>%
  select(-prob_1star, -prob_2star, -prob_3star, -prob_4star, -prob_5star)

ggplot(restaurant_review_full_with_bert,aes(bert_score))+
  geom_bar(fill= "#800020",color="white")+
  geom_text(stat = "count", aes(label = after_stat(count)), 
            vjust = -0.5, size = 3.2)+
  labs(title = "Distribution of bert_score",
       y = "Frequency",
       x = "bert_score")

# ä½¿ç”¨bertåŒºåˆ†valence positive(1)/neutral(0)/negative(-1)
restaurant_review_full_with_bert <- restaurant_review_full_with_bert %>% 
  mutate(bert_valence = case_when(bert_score > 3 ~  1,
                                  bert_score < 3 ~ -1,
                                  bert_score ==3 ~  0,
                                  TRUE ~ NA))

summary(restaurant_review_full_with_bert[,c(30:42)])

##########################
# Descriptive Analysis --------------
##########################

summary_counts <- restaurant_review_full_with_bert %>%
  summarise(
    total_reviews = n(),
    unique_businesses = n_distinct(business_id),
    unique_users = n_distinct(user_id),
    unique_states = n_distinct(state)
  )
summary_counts

# è®¡ç®—æ¯ä¸ªå˜é‡çš„å‡å€¼å’Œæ ‡å‡†å·®
descriptive_stats <- restaurant_review_full_with_bert %>%
  select(all_of(c("useful", "new_reviews_3m", 
                   "bert_valence", "membership_length",
                  "user_avg_useful", "average_stars", "elite_years"))) %>%
  summarise(across(everything(), list(mean = ~mean(., na.rm = TRUE),
                                      sd = ~sd(., na.rm = TRUE))))
descriptive_stats

# é€‰æ‹©æŒ‡å®šåˆ—
selected_cols <- restaurant_review_full_with_bert %>%
  select(as.numeric(all_of(c("useful", "new_reviews_3m", 
                             "relevant", "comprehensive", "clarity", "bert_objective", "word_count", 
                             "bert_valence", "membership_length","elite_years", "user_avg_useful")))) 

# å°†é€»è¾‘å‹å˜é‡è½¬æ¢ä¸ºæ•°å€¼å‹
selected_cols <- selected_cols %>%
  mutate(across(where(is.logical), as.numeric))

# è®¡ç®—ç›¸å…³çŸ©é˜µï¼ˆè‡ªåŠ¨æ’é™¤ NAï¼‰
cor_matrix <- cor(selected_cols, use = "pairwise.complete.obs")

# ç»˜åˆ¶ç›¸å…³å›¾
corrplot(cor_matrix, method = "color", type = "upper",
         tl.col = "black", tl.srt = 45, addCoef.col = "black",
         number.cex = 0.6, col = colorRampPalette(c("steelblue", "white", "tomato"))(200))

# çœ‹çœ‹usefulçš„distribution
ggplot(restaurant_review_full_with_bert,aes(useful))+
  geom_bar(fill= "#800020",color="white")+
  geom_text(stat = "count", aes(label = after_stat(count)), 
            vjust = -0.5, size = 3.2)+
  labs(title = "Distribution of useful",
       y = "Frequency",
       x = "useful")

# çœ‹çœ‹EBçš„distribution
ggplot(restaurant_review_full_with_bert,aes(new_reviews_3m))+
  geom_bar(fill= "#800020",color="white")+
  geom_text(stat = "count", aes(label = after_stat(count)), 
            vjust = -0.5, size = 3.2)+
  labs(title = "Distribution of new reviews 3 months after each review",
       y = "Frequency",
       x = "useful")
summary(restaurant_review_full_with_bert$new_reviews_3m)

# çœ‹çœ‹word countçš„distribution
ggplot(restaurant_review_full_with_bert,aes(word_count))+
  geom_histogram(fill= "#800020",color="white")+
  labs(title = "Distribution of word count",
       y = "Frequency",
       x = "useful")
summary(restaurant_review_full_with_bert$new_reviews_3m)

# çœ‹çœ‹valenceçš„distribution
ggplot(restaurant_review_full_with_bert, aes(x = bert_valence)) +
  geom_bar(fill = "#800020", color = "white", width = 0.6) +
  geom_text(stat = "count", aes(label = after_stat(count)),
            vjust = -0.5, size = 4) +
  labs(title = "Distribution of Valence",
       x = "Valence Category",
       y = "Frequency") +
  theme_minimal()

# ç»Ÿè®¡æ¯ä¸ªå·çš„è¯„è®ºæ•°ï¼Œå¹¶æŒ‰æ•°é‡æ’åº
state_review_counts <- restaurant_review_full_with_bert %>%
  group_by(state) %>%
  summarise(review_count = n()) %>%
  arrange(desc(review_count))

# ç»˜åˆ¶å›¾è¡¨
ggplot(state_review_counts, aes(x = reorder(state, -review_count), y = review_count)) +
  geom_bar(stat = "identity", fill = "#800020") +
  geom_text(aes(label = review_count), 
            vjust = -0.5, size = 3.2) +
  labs(title = "Number of Reviews per State",
       x = "State",
       y = "Total Reviews") +
  theme_minimal()

# æ¯ä¸ªå·çš„å¹³å‡RU
ggplot(state_means, aes(x = reorder(state, -avg_business_stars), y = avg_business_stars)) +
  geom_bar(stat = "identity", fill = "#800020") +
  geom_text(aes(label = round(avg_business_stars, 2)), vjust = -0.5, size = 3.2) +
  labs(title = "Average Business Stars per State",
       x = "State",
       y = "Average Stars") +
  theme_minimal()

# æ¯ä¸ªå·çš„å¹³å‡EB
ggplot(state_means, aes(x = reorder(state, -avg_new_reviews_3m), y = avg_new_reviews_3m)) +
  geom_bar(stat = "identity", fill = "#800020") +
  geom_text(aes(label = round(avg_new_reviews_3m, 1)), vjust = -0.5, size = 3.2) +
  labs(title = "Average New Reviews in 3 Months per State",
       x = "State",
       y = "Avg. New Reviews") +
  theme_minimal()

# æ¯ä¸ªå·çš„å¹³å‡åº—é“ºæœ‰ç”¨æŠ•ç¥¨
ggplot(state_means, aes(x = reorder(state, -avg_useful), y = avg_useful)) +
  geom_bar(stat = "identity", fill = "#800020") +
  geom_text(aes(label = round(avg_useful, 1)), vjust = -0.5, size = 3.2) +
  labs(title = "Average Review Usefulness per State",
       x = "State",
       y = "Avg. Useful Score") +
  theme_minimal()


# ä¸­å¿ƒåŒ–RUå’Œvalence,å¹¶åˆ›å»ºäº¤äº’é¡¹
restaurant_review_full_with_bert <- restaurant_review_full_with_bert %>%
  mutate(
    useful_c = scale(useful, center = TRUE, scale = FALSE),
    valence_c = scale(bert_valence, center = TRUE, scale = FALSE),
    int_use_val = useful_c * valence_c
  )

# è§‚æµ‹å˜é‡è¿›è¡Œæ ‡å‡†åŒ–
restaurant_review_full_with_bert <- restaurant_review_full_with_bert %>%
  mutate(
    review_count = as.numeric(scale(review_count, center = TRUE, scale = TRUE)),
  )

# å°†æ–‡å­—è½¬æ¢æˆå› å­
restaurant_review_full_with_bert$state <- as.factor(restaurant_review_full_with_bert$state)
restaurant_review_full_with_bert$year_quarter <- as.factor(restaurant_review_full_with_bert$year_quarter)
str(restaurant_review_full_with_bert)
summary(restaurant_review_full_with_bert)

# ä¸€æ¬¡æ€§ dummy åŒ– state å’Œ year_quarterï¼ˆå…± 17 + 20 ä¸ª dummyï¼‰
#install.packages("fastDummies")
restaurant_review_full_with_bert <- fastDummies::dummy_cols(
  restaurant_review_full_with_bert,
  select_columns = c("state", "year_quarter"),
  remove_selected_columns = TRUE,  # å»æ‰åŸå§‹åˆ—
  remove_first_dummy = TRUE        # é¿å…å¤šé‡å…±çº¿æ€§ï¼ˆç•™å‡º reference ç±»åˆ«ï¼‰
)
# æŠŠæ‰€æœ‰åˆ—åä¸­çš„ "-" æ›¿æ¢æˆ "_"ï¼ˆåªæ›¿æ¢åˆ—åï¼Œä¸æ”¹å†…å®¹ï¼‰
names(restaurant_review_full_with_bert) <- gsub("-", "_", names(restaurant_review_full_with_bert))
colSums(restaurant_review_full_with_bert[,37:70])

################
# pre-test for NBR -----------
################
# å‡å€¼ vs æ–¹å·®æ£€éªŒï¼ˆåˆ¤æ–­æ˜¯å¦è¿‡åº¦ç¦»æ•£ï¼‰
set.seed(1234)  # å¯å¤ç°çš„åˆ’åˆ†
n_total <- nrow(restaurant_review_full_with_bert)
train_indices <- sample(1:n_total, size = 0.8 * n_total)

train_df <- restaurant_review_full_with_bert[train_indices, ]
test_df  <- restaurant_review_full_with_bert[-train_indices, ]

mean(train_df$new_reviews_3m)
var(train_df$new_reviews_3m)
dispersion_ratio <- var(train_df$new_reviews_3m) / mean(train_df$new_reviews_3m)
# å¦‚æœ Variance >> Meanï¼ˆæ¯”å¦‚ ratio > 1.5~2ï¼‰ï¼Œè¯´æ˜æœ‰æ˜æ˜¾è¿‡åº¦ç¦»æ•£, Dispersion_Ratio = 45 æ‰€ä»¥é€šè¿‡

# 0çš„æ¯”ä¾‹åˆ†æï¼ˆåˆ¤æ–­æ˜¯å¦é›¶è†¨èƒ€ï¼‰
zero_rate <- mean(train_df$new_reviews_3m == 0)
cat("Proportion of 0s:", round(zero_rate * 100, 2), "%\n")
# 0åªå äº†5.67%,ä¸éœ€è¦0è†¨èƒ€

# å¯¹æ¯”ä¸€ä¸‹æ³Šæ¾å’Œè´ŸäºŒ

pois_model <- glm(new_reviews_3m ~ useful_c + valence_c + int_use_val,
                  family = poisson, data = train_df)
nb_model <- glm.nb(new_reviews_3m ~ useful_c + valence_c + int_use_val,
                   data = train_df)
AIC(pois_model, nb_model)

#å¯¹æ¯”ä¸€ä¸‹è´ŸäºŒå’Œé›¶è†¨èƒ€è´ŸäºŒ
zinb <- zeroinfl(
  new_reviews_3m ~ useful_c + valence_c + int_use_val | 1,
  data = train_df,
  dist = "negbin"
)
vuong(zinb, nb_model)

#Raw æ£€éªŒæ— æ˜¾è‘—å·®å¼‚ â†’ æ¨¡å‹ä¹‹é—´æ— æ³•åŒºåˆ†ï¼›
#AIC/BIC æ ¡æ­£åéƒ½æ˜¾è‘— < 0ï¼š
#z å€¼ä¸ºè´Ÿ â†’ NB æ›´å¥½ï¼›
#p < 0.001 â†’ å·®å¼‚éå¸¸æ˜¾è‘—ï¼›
#ç”¨ Negative Binomial æ›´åˆé€‚ï¼ŒZINB åè€Œè¿‡æ‹Ÿåˆ

# æ„å»ºæ¨¡å‹
# NBR_1ï¼šä¸»æ•ˆåº”
nb1 <- glm.nb(new_reviews_3m ~ useful_c + valence_c, data = train_df)
summary(nb1)

# NBR_2ï¼šåŠ äº¤äº’é¡¹
nb2 <- glm.nb(new_reviews_3m ~ useful_c + valence_c + int_use_val, data = train_df)
summary(nb2)

# NBR_3ï¼šåŠ å…¥æ§åˆ¶å˜é‡å’Œdummy
nb3 <- glm.nb(new_reviews_3m ~ useful_c + valence_c + int_use_val + review_count +
                state_AZ + state_CA + state_DE + state_FL + state_ID + state_IL + state_IN +
                state_LA + state_MO +  state_NJ + state_NV + state_PA + state_TN +
                year_quarter_2017_Q2 + year_quarter_2017_Q3 + year_quarter_2017_Q4 +
                year_quarter_2018_Q1 + year_quarter_2018_Q2 + year_quarter_2018_Q3 + year_quarter_2018_Q4 +
                year_quarter_2019_Q1 + year_quarter_2019_Q2 + year_quarter_2019_Q3 + year_quarter_2019_Q4 +
                year_quarter_2020_Q1 + year_quarter_2020_Q2 + year_quarter_2020_Q3 + year_quarter_2020_Q4 +
                year_quarter_2021_Q1 + year_quarter_2021_Q2 + year_quarter_2021_Q3,
              data = train_df)
summary(nb3)

# Calculate McFadden RÂ² + Nagelkerke RÂ²
# æ¨¡å‹ 1ï¼šä¸»æ•ˆåº”
cat("âœ… Model 1 (Main Effects):\n")
r2_nb1 <- pR2(nb1)
cat("ğŸ“˜  McFadden's RÂ²: ", round(r2_nb1["McFadden"], 4), "\n")
cat("ğŸ“—  Nagelkerke RÂ²:", round(r2_nb1["r2CU"], 4), "\n\n")  # r2CU æ˜¯ Nagelkerke

# æ¨¡å‹ 2ï¼šå«äº¤äº’é¡¹
cat("âœ… Model 2 (Interaction):\n")
r2_nb2 <- pR2(nb2)
cat("ğŸ“˜  McFadden's RÂ²: ", round(r2_nb2["McFadden"], 4), "\n")
cat("ğŸ“—  Nagelkerke RÂ²:", round(r2_nb2["r2CU"], 4), "\n\n")

# æ¨¡å‹ 3ï¼šå«æ§åˆ¶å˜é‡å’Œ dummy
cat("âœ… Model 3 (Full Model with Controls):\n")
r2_nb3 <- pR2(nb3)
cat("ğŸ“˜  McFadden's RÂ²: ", round(r2_nb3["McFadden"], 4), "\n")
cat("ğŸ“—  Nagelkerke RÂ²:", round(r2_nb3["r2CU"], 4), "\n\n")

# å¯¹æµ‹è¯•é›†/è®­ç»ƒé›†è¿›è¡Œé¢„æµ‹ ------------------------------
test_df$predicted_reviews <- predict(nb3, newdata = test_df, type = "response")
train_df$predicted_reviews <- predict(nb3, newdata = train_df, type = "response")

# å¯è§†åŒ–é¢„æµ‹æ•ˆæœ
ggplot(test_df, aes(x = predicted_reviews, y = new_reviews_3m)) +
  geom_point(alpha = 0.15) +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  labs(title = "Predicted vs. Actual New Reviews (NB)",
       x = "Predicted New Reviews",
       y = "Actual New Reviews") +
  theme_minimal()

# MAEã€RMSEã€Hit Rate --------------------------
library(Metrics)
# MAE, RMSE, Hit Rate for train
mae_train <- mae(train_df$new_reviews_3m, train_df$predicted_reviews)
rmse_train <- rmse(train_df$new_reviews_3m, train_df$predicted_reviews)
hit_train <- mean(ifelse(train_df$new_reviews_3m > median(train_df$new_reviews_3m), 1, 0) ==
                    ifelse(train_df$predicted_reviews > median(train_df$predicted_reviews), 1, 0))

# MAE, RMSE, Hit Rate for test
mae_test <- mae(test_df$new_reviews_3m, test_df$predicted_reviews)
rmse_test <- rmse(test_df$new_reviews_3m, test_df$predicted_reviews)
hit_test <- mean(ifelse(test_df$new_reviews_3m > median(test_df$new_reviews_3m), 1, 0) ==
                   ifelse(test_df$predicted_reviews > median(test_df$predicted_reviews), 1, 0))

# è¾“å‡º
cat("ğŸ“˜ Train MAE:", round(mae_train, 2), " | Test MAE:", round(mae_test, 2), "\n")
cat("ğŸ“˜ Train RMSE:", round(rmse_train, 2), " | Test RMSE:", round(rmse_test, 2), "\n")
cat("ğŸ“˜ Train Hit Rate:", round(hit_train, 4), " | Test Hit Rate:", round(hit_test, 4), "\n")

# æ¨¡æ‹Ÿäº¤äº’å›¾ -----------------------------------
interaction_grid <- expand.grid(
  valence_c = seq(-1.5, 0.5, length.out = 50),
  useful_c = c(0, 40, 80, 120)
)
interaction_grid$int_use_val <- interaction_grid$useful_c * interaction_grid$valence_c
interaction_grid$review_count <- mean(train_df$review_count, na.rm = TRUE)
for (col in grep("^state_|^year_quarter_", names(train_df), value = TRUE)) {
  interaction_grid[[col]] <- 0
}
interaction_grid$predicted_reviews <- predict(nb3, newdata = interaction_grid, type = "response")
interaction_grid$useful_level <- factor(interaction_grid$useful_c)

ggplot(interaction_grid, aes(x = valence_c, y = predicted_reviews, color = useful_level)) +
  geom_line(size = 1.2) +
  labs(
    title = "Moderation Effect: Usefulness x Valence (NB)",
    x = "Valence (centered)",
    y = "Predicted New Reviews",
    color = "Usefulness"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("0" = "#a6cee3", "40" = "#1f78b4", "80" = "#33a02c", "120" = "#e31a1c"))

# æ®‹å·®å›¾ï¼ˆNBï¼‰ ----------------------------------
resid_df <- data.frame(
  fitted = predict(nb3, type = "response"),
  resid = residuals(nb3, type = "pearson")
)

ggplot(resid_df, aes(x = fitted, y = resid)) +
  geom_point(alpha = 0.2, shape = 1) +
  geom_hline(yintercept = 0, color = "red") +
  coord_cartesian(ylim = c(-2, 16)) +
  labs(
    title = "Residuals vs Fitted (NB)",
    x = "Predicted values",
    y = "Pearson residuals"
  ) +
  theme_minimal()

# å¤šé‡å…±çº¿æ€§æ£€æµ‹ --------------------------------
vif(glm.nb(new_reviews_3m ~ useful_c + valence_c + int_use_val + review_count, data = train_df))
